import inspect
import os
import pathlib
import sys
import typing
import re

import numpy as np
import pandas as pd
import psutil  # type: ignore
from IPython.core.getipython import get_ipython
from numpy.typing import NDArray


def export(function):
    module_globals = inspect.stack()[1][0].f_globals
    if "__all__" not in module_globals:
        module_globals["__all__"] = []

    module_globals["__all__"].append(function.__name__)
    # print(function.__name__, module_globals["__name__"], module_globals["__all__"])

    return function


def load_slope_values(path: str) -> tuple[int, NDArray]:
    data = np.load(path)

    return data[0], data[1:]


def get_memory() -> float:
    """Return memory usage in MB"""
    process = psutil.Process(os.getpid())
    memory_usage = process.memory_info().rss  # in bytes
    return memory_usage / (1024 ** 2)


def is_notebook() -> bool:
    try:
        # Check if the environment is an IPython shell (which includes Jupyter)
        if 'ipykernel' in sys.modules or 'IPython' in sys.modules:
            if get_ipython().__class__.__name__ == 'ZMQInteractiveShell':
                # We're in a Jupyter notebook
                return True
        return False
    except NameError:
        return False  # If 'IPython' is not available, we're not in a notebook


def get_hist(sample: typing.Sequence, bins: int | typing.Iterable | None = None, **kwargs) -> tuple[
    NDArray[np.float64], [np.float64]]:
    edges = np.array(range(
        np.floor(sample).min().astype(int),
        np.ceil(sample).max().astype(int) + 1)) - 0.5

    default = {
        "density": True,
    }
    default.update(kwargs)

    bins, edges = np.histogram(sample, bins=bins or edges, **default)

    return 0.5 * (edges[1:] + edges[:-1]), bins


def get_2d_hist(x_sample: typing.Sequence, y_sample: typing.Sequence, **kwargs
                ) -> tuple[NDArray[np.float64], [np.float64], NDArray[np.float64]]:
    edges = []
    for sample in [x_sample, y_sample]:
        e = np.array(range(
            np.floor(sample).min().astype(int),
            np.ceil(sample).max().astype(int) + 1)) - 0.5
        edges.append(e)

    hist_kwargs = {
        "density": True,
    }
    hist_kwargs.update(kwargs)

    bins, _, _ = np.histogram2d(x_sample, y_sample, bins=(edges[0], edges[1]), **hist_kwargs)
    x = 0.5 * (edges[0][1:] + edges[0][:-1])
    y = 0.5 * (edges[1][1:] + edges[1][:-1])

    return x, y, bins


def get_system_params_from_name(name: str) -> dict[str, any]:
    """

    Get the system parameters from the folder name generated by SandpileND.run_multiple_samples
    function.

    :param name: name of the folder
    :return: Dictionary of the system parameters
    """

    values = name.split("_")

    result = dict()
    result["dimension"] = int(values[0][1:])
    result["linear_grid_size"] = int(values[1][1:])
    result["critical_slope"] = int(values[2][1:])

    if values[3] == "op":
        result["boundary_condition"] = "open"
    elif values[3] == "cl":
        result["boundary_condition"] = "closed"
    else:
        raise ValueError(f"unknown boundary condition {values[3]}")

    if values[4] == "co":
        result["perturbation"] = "conservative"
    elif values[4] == "nco":
        result["perturbation"] = "non conservative"
    else:
        raise ValueError(f"unknown perturbation {values[4]}")

    return result


def get_short_params(dct: dict[str, any]) -> dict[str, any]:
    val = dct["boundary_condition"]
    dct["boundary_condition"] = "op" if val == "open" else "cl"

    val = dct["perturbation"]
    dct["perturbation"] = "co" if val == "conservative" else "nco"

    return dct


def load_combine_avalanche_data_samples(data_dir: str | pathlib.Path, with_dissipation: bool = True,
                                        down_casting: bool = True
                                        ) -> pd.DataFrame:
    if isinstance(data_dir, str):
        data_dir = pathlib.Path(data_dir)
    elif not isinstance(data_dir, pathlib.Path):
        raise TypeError("data_dir must be pathlib.Path or str")

    df = pd.DataFrame({})
    for file in data_dir.glob("*.avalanche.csv.gz"):
        dfn = pd.read_csv(file)
        dfn["sample"] = int(re.findall(r"\d+", file.name)[0])
        df = pd.concat([df, dfn], axis=0)

    dp_rates = []
    if with_dissipation:
        for file in data_dir.glob("*.avalanche.npz"):
            data = np.load(file)
            dp_rates.extend([data[f"arr_{i}"] for i in range(len(data))])

        df["dissipation_rate"] = dp_rates

    return df.reset_index(drop=True).set_index(["sample", "time_step"])
    # return df.reset_index()
